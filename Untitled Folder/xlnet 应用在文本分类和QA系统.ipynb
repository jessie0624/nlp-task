{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练语言模型：\n",
    "- 作为特征提取器\n",
    "- 作为encoder参数下游任务微调使用上非常类似，差别是后者在训练过程中原预训练语言模型的参数也允许优化。\n",
    "\n",
    "主要内容：\n",
    "- 以XLNET介绍 HuggingFace transformers 组件的使用套路\n",
    "- 以XLNET为例介绍如何接下游任务的文本分类和抽取式问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f465dd3bebf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install transformers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.functional as F \n",
    "!pip install transformers\n",
    "from transformers import XLNetModel, XLNetTokenizer, XLNetConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model = XLNetModel.from_pretrained('xlnet-base-cased',\n",
    "                                  output_hidden_states=True,\n",
    "                                  output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本地加载XLNET模型\n",
    "# MODEL_PATH = r'D:\\data\\nlp\\xlnet-model/'\n",
    "# config = XLNetConfig.from_json_file(os.path.join(MODEL_PATH, 'xlnet-base-cased-config.json'))\n",
    "# config.output_hidden_states = True\n",
    "# config.output_attentions = True\n",
    "\n",
    "# tokenizer = XLNetTokenizer(os.path.join(MODEL_PATH, 'xlnet-base-cased-spiece.model'))\n",
    "# model = XLNetModel.from_pretrained(MODEL_PATH, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 句子到token_id的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用tokenizer将原始的句子准备成模型输入\n",
    "sentence= 'This is an interesting review session'\n",
    "\n",
    "# tokenization\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "\n",
    "# 将token 转化为ID\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens id: {}\".format(tokens_ids))\n",
    "\n",
    "# 添加特殊token:<cls>, <sep>\n",
    "tokens_ids = tokenizer.build_inputs_with_special_tokens(tokens_ids)\n",
    "\n",
    "# 准备成pytorch tensor\n",
    "tokens_pt = torch.tensor([tokens_ids])\n",
    "print(\"Tokens Pytorch: {}\".format(tokens_pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一条龙服务\n",
    "tokens_pt2 = tokenizer(sentence, return_tensors='pt')\n",
    "print(\"Tokens Pytorch: {}\".format(tokens_pt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批处理\n",
    "sentences = ['The ultimate answer to life, universe and time is 42.',\n",
    "            'Take a towel for a space travel.']\n",
    "print(\"Batch tokenization:\\n\", tokenizer(sentences)[\"input_ids\"])\n",
    "print(\"With padding:\\n\", tokenizer(sentences, padding=True)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入句子对\n",
    "multi_seg_input = tokenizer(\"This is segment A\", \"This is segment B.\")\n",
    "print(\"Multi segment token (str): {}\".format(tokenizer.convert_ids_to_tokens(multi_seg_input['input_ids'])))\n",
    "print(\"Multi segment token (int): {}\".format(multi_seg_input['input_ids']))\n",
    "print(\"Multi segment type       : {}\".format(multi_seg_input['token_type_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 模型encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认情况下模型是model.eval()模式，下面使用encode输入的句子\n",
    "print(\"Is training mode?\", model.training)\n",
    "sentence = \"The ultimate answer to life, universe and time is 42.\"\n",
    "tokens_pt = tokenizer(sentence, return_tensor='pt')\n",
    "print('Token:'.format(tokenizer.convert_ids_to_tokens(tokens_pt[\"input_ids\"][0])))\n",
    "\n",
    "final_layer_h, all_layer_h, attentions = model(**tokens_pt)\n",
    "print(torch.sum(final_layer_h - all_layer_h[-1]).item())\n",
    "\n",
    "final_layer_h.shape, len(all_layer_h), len(attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 下游任务\n",
    "\n",
    "### 例1 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a13109125c4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mXLNetSeqSummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     def __init__(self, how='cls',\n\u001b[0;32m      3\u001b[0m                  \u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m768\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                  \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                  \u001b[0mfirst_dropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class XLNetSeqSummary(nn.Module):\n",
    "    def __init__(self, how='cls',\n",
    "                 hidden_size = 768,\n",
    "                 activation = None,\n",
    "                 first_dropout = None,\n",
    "                 last_dropout = None):\n",
    "        super().__init__()\n",
    "        self.how = how\n",
    "        self.summary = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = activation if activation else nn.GELU()\n",
    "        self.first_dropout = first_dropout if first_dropout else nn.Dropout(0.5)\n",
    "        self.last_dropout = last_dropout if last_dropout else nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        对隐状态序列池化或返回cls处的表示，作为句子的encoding.\n",
    "        Args: hidden_states: XLNET 模型输出的最后层隐状态序列。\n",
    "        Returns: 句子向量表示\n",
    "        \"\"\"\n",
    "        if self.how == 'cls':\n",
    "            output = hidden_states[:, -1]\n",
    "        elif self.how == 'mean':\n",
    "            output = hidden_states.mean(dim=1)\n",
    "        elif self.how == 'max':\n",
    "            output = hidden_states.max(dim=1)\n",
    "        else:\n",
    "            raise Exception(\"Summary type {} not implemted\".format(self.how))\n",
    "        \n",
    "        output = self.first_dropout(output)\n",
    "        output = self.summary(output)\n",
    "        output = self.activation(output)\n",
    "        output = self.last_dropout(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLNetSentenceClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, xlnet_model, d_model=768):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.d_model = d_model\n",
    "        self.transformer = xlnet_model\n",
    "        self.sequence_summary = XLNetSeqSummary('cls', d_model, nn.GELU())\n",
    "        self.logits_proj = nn.Linear(d_model, num_labels)\n",
    "    \n",
    "    def forward(self, model_inputs):\n",
    "        transformer_outputs = self.transformer(**model_inputs)\n",
    "        output = transformer_outputs[0]\n",
    "        output = self.sequence_summary(output)\n",
    "        logits = self.logits_proj(output)\n",
    "        return logits\n",
    "\n",
    "def get_loss(criterion, logits, labels):\n",
    "    return criterion(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证forward 和反向传播\n",
    "sentences = ['The ultimate answer to life, universe and time is 42.',\n",
    "             'Take a towel for a space travel.']\n",
    "\n",
    "labels = torch.LongTensor([0, 1])\n",
    "\n",
    "# 实例化各个模块\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "classifier = XLNetSentenceClassifier(2, model)\n",
    "optimizer = torch.optim.AdamW(classifier.parameters())\n",
    "\n",
    "# forward + loss\n",
    "classifier.train()\n",
    "optimizer.zero_grad()\n",
    "logits = classifier(tokenizer(sentences, padding=True, return_tensors='pt'))\n",
    "loss = get_loss(criterion, logits, labels)\n",
    "print('loss: ', loss.item())\n",
    "\n",
    "# backward step\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(\"=\"*25)\n",
    "print(\"Confirm that the gradients are computed for the original XLNet parameters.\\n\")\n",
    "for param in classifier.parameters():\n",
    "    print(param.shape, param.grad.sum() if not param.grad is None else param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例2 抽取式问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnsStartLogits(nn.Module):\n",
    "    \"用于预测每个token是否为答案span开始位置\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, hidden_states, p_mask=None):\n",
    "        x = self.linear(hidden_states).squeeze(-1)\n",
    "        if p_mask is not None:\n",
    "            x = x * (1-p_mask) - 1e30 * p_mask\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnsEndLogits(nn.Module):\n",
    "    \"用于预测每个token是否为答案span结束位置，符合直觉。conditioned on 开始位置\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    def forward(self, hidden_states, start_states, p_mask=None):\n",
    "        x = self.layer(torch.cat([hidden_states, start_states], dim=-1))\n",
    "        x = x.squeeze(-1)\n",
    "        if p_mask is not None:\n",
    "            x = x*(1-p_mask) - 1e30 * p_mask\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLNetQuestionAnswering(nn.Module):\n",
    "    def __init__(self, num_labels, xlnet_model, d_model=768,\n",
    "                top_k_start=2, top_k_end=2):\n",
    "        super().__init__()\n",
    "        self.transformer = xlnet_model\n",
    "        self.start_logits = AnsStartLogits(d_model)\n",
    "        self.end_logits = AnsEndLogits(d_model) # d_model就是hiddensize\n",
    "        self.top_k_start = top_k_start\n",
    "        self.top_k_end = top_k_end\n",
    "    \n",
    "    def forward(self, model_inputs, p_mask=None, start_positions=None):\n",
    "        \"\"\"\n",
    "        p_mask 可选的mask, 被mask的位置不可能存在答案（eg:[CLS][PAD][QUES]）\n",
    "                1.0表示应当被mask, 0.0反之为不被mask的值\n",
    "        start_positions 正确答案标注的开始位置，训练时需要输入模型以利用\n",
    "                teacher forcing计算end_logits. \n",
    "                Inference时 不需要输入，beam search 返回top k个开始和结束位置。\n",
    "        \"\"\"\n",
    "        transformer_outputs = self.transformer(**model_inputs)\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n",
    "        if not start_positions is None:\n",
    "            # 在训练时利用teacher forcing trick训练end_logits\n",
    "            slen, hsz = hidden_states.shape[-2:]\n",
    "            start_positions = start_positions.expand(-1, -1, hsz) # shape: (bsz, 1, hsz)\n",
    "            start_states = hidden_states.gather(-2, start_positions) # bsz,1,hsz\n",
    "            start_states = start_states.expand(-1, slen, -1) # shape: bsz, slen, hsz\n",
    "            end_logits = self.end_logits(hidden_states,\n",
    "                                        start_states=start_states,\n",
    "                                        p_mask_p_mask)\n",
    "            return start_logits, end_logits\n",
    "        else:\n",
    "            # 在Inference 时 利用Beam Search求end_logit\n",
    "            bsz, slen, hsz = hidden_states.size() # batchsize, seq_len, hidden_size\n",
    "            start_probs = torch.softmax(start_logits, dim=-1)\n",
    "            start_top_probs, start_top_index = torch.topk(start_probs, self.top_k_start, dim=-1)\n",
    "            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz)#(bsz, top_k_start, hsz)\n",
    "            start_states = torch.gather(hidden_states, -2, start_top_index_exp)\n",
    "            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1)\n",
    "            \n",
    "            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(start_states)\n",
    "            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n",
    "            end_logits = self.end_logits(hidden_states_expanded,\n",
    "                                        start_states = start_states,\n",
    "                                        p_mask = p_mask)\n",
    "            end_probs = torch.softmax(end_logits, dim=1)#shape: bsz, slen, top_k_start\n",
    "            end_top_probs, end_top_index = torch.topk(end_probs, self.top_k_end, dim=1) \n",
    "            # shape: bsz, top_k_end, top_k_start\n",
    "            end_top_probs = torch.transpose(end_top_probs, 2, 1)\n",
    "            # bsz, top_k_start, top_k_end\n",
    "            end_top_index = torch.transpose(end_top_index, 2, 1)\n",
    "            # bsz, top_k_start, top_k_end\n",
    "            \n",
    "            end_top_probs = end_top_probs.reshape(-1, self.top_k_start * self.top_k_end)\n",
    "            end_top_index = end_top_index.reshape(-1, self.top_k_start * self.top_k_end)\n",
    "            \n",
    "            return start_top_probs, start_top_index, end_top_probs, end_top_index, start_logits, end_logits\n",
    "            \n",
    "\n",
    "def get_loss(criterion, start_logits, start_positions, end_logits, end_positions):\n",
    "    start_loss = criterion(start_logits, start_positions)\n",
    "    end_loss = criterion(end_logits, end_positions)\n",
    "    return (start_loss + end_loss)/2\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检测用于训练的forward和backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"\n",
    "    Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "    architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
    "    Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "    TensorFlow 2.0 and PyTorch.\n",
    "    \"\"\"\n",
    "questions = [\n",
    "    \"How many pretrained models are available in Transformers?\",\n",
    "    \"What does Transformers provide?\",\n",
    "    \"Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "start_positions = torch.LongTensor([95, 36, 110])\n",
    "end_positions = torch.LongTensor([97, 88, 123])\n",
    "p_mask = [[1]*12 + [0]*(125-14) + [1,1],\n",
    "          [1]*7  + [0]*(120- 9) + [1,1],\n",
    "          [1]*12 + [0]*(125-14) + [1,1]]\n",
    "\n",
    "neg_log_loss = nn.CrossEntropyLoss()\n",
    "q_answer = XLNetQuestionAnswering(2, model, 768, 2, 2)\n",
    "optimizer = torch.optim.AdamW(q_answer.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_answer.train()\n",
    "optimizer.zero_grad()\n",
    "for ith, question in enumerate(questions):\n",
    "    start_logits, end_logits = q_answer(\n",
    "        tokenizer(question,\n",
    "                  context,\n",
    "                  add_special_tokens=True,\n",
    "                  return_tensors='pt'),\n",
    "        p_mask = torch.ByteTensor(p_mask[ith]),\n",
    "        start_positions = start_positions[ith].view(1,1,1)\n",
    "        )\n",
    "    loss = get_loss(criterion,\n",
    "                   start_logits,\n",
    "                   start_positions[ith].view(-1),\n",
    "                   end_logits,\n",
    "                   end_positions[ith].view(-1))\n",
    "    print(\"\\n True Start:{}, True End:{}\\n Pred Start Prob:{},Pred End Prob: {}\\nPred Max Start: {}, Pred Max End: {}\\nPred Max Start Prob: {}, Pred Max end Prob:{}\\nLoss: {}\\n\".format(\n",
    "        start_positions[ith].item(),\n",
    "        end_positions[ith].item(),\n",
    "        torch.sigmoid(start_logits[:, start_positions[ith]]).item(),\n",
    "        torch.sigmoid(end_logits[:, end_positions[ith]]).item(),\n",
    "        torch.argmax(start_logits).item(),\n",
    "        torch.argmax(end_logits).item(),\n",
    "        torch.sigmoid(torch.max(start_logits)).item(),\n",
    "        torch.sigmodi(torch.max(end_logits)).item(),\n",
    "        loss.item()\n",
    "    ))\n",
    "    print(\"=\"*25)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(\"\\n Confirm that the gradients are computed for the original XLNET parameters\")\n",
    "for param in q_answer.parameters():\n",
    "    print(param.shape, param.grad.sum() if not param.grad is None else param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference的forward以及实现Beam Search decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def decode(start_probs, end_probs, topk):\n",
    "    \"\"\"\n",
    "    给定beam中预测的开始和结束概率，搜索topk个最佳答案\n",
    "    \"\"\"\n",
    "    top_k_start = start_probs.shape[-1]\n",
    "    top_k_end = end_probs.shape[-1]//top_k_start\n",
    "    #计算每一个（start end）对的分数，P(Start, end|sentence) = P(start|sentence) * P(end|start, sentence)\n",
    "    joint_probs = dict()\n",
    "    for i in range(top_k_start):\n",
    "        for j in range(top_k_end):\n",
    "            end_idx = i * top_k_end + j\n",
    "            joint_probs[(i, end_idx)] = start_probs[i] * end_probs[end_idx]\n",
    "    id_pairs, probs = zip(*sorted(joint_probs.items(), key=lambda kv: kv[1], reverse=True)[:topk])\n",
    "    start_ids, end_ids = zip(*id_pairs)\n",
    "    return start_ids, end_ids, probs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "# inference\n",
    "context = r\"\"\"\n",
    "    Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "    architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
    "    Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "    TensorFlow 2.0 and PyTorch.\n",
    "    \"\"\"\n",
    "questions = [\n",
    "    \"How many pretrained models are available in Transformers?\",\n",
    "    \"What does Transformers provide?\",\n",
    "    \"Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "q_answer.eval()\n",
    "for ith, question in enumerate(questions):\n",
    "    inputs = tokenizer(question, context, add_special_tokens=True, return_tensors='pt')\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    start_probs, start_index, end_probs, end_index, start_logits, end_logits = q_answer(\n",
    "        inputs, p_mask=torch.ByteTensor(p_mask[ith]))\n",
    "    pred_starts, pred_ends, probs = decode(\n",
    "        start_probs.detach().squeeze().numpy(),\n",
    "        end_probs.detach().squeeze().numpy(),\n",
    "        2)\n",
    "    # 只打印一个答案\n",
    "    start = start_index[:, pred_starts[0]].item()\n",
    "    end = end_index[:, pred_ends[0]].item()\n",
    "    print(\"=\"*25)\n",
    "    print(\"True start: {}, True end: {}\".format(\n",
    "        start_positions[ith].item(),\n",
    "        end_positions[ith].item()\n",
    "        ))\n",
    "    print(\"Max answer prob: {:0.8f}, start idx: {}, end idx: {}\".format(\n",
    "        probs[0],\n",
    "        start,\n",
    "        end,\n",
    "    ))\n",
    "    print(\"-\"*25)\n",
    "    print(\"Question: '{}'\".format(question))\n",
    "    print(\"Answer: '{}'\".format(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[start:end]))))\n",
    "    print(\"=\"*25)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
